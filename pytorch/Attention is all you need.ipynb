{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59460955",
   "metadata": {},
   "source": [
    "# Attention is all you need."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0222e7dc",
   "metadata": {},
   "source": [
    "https://arxiv.org/abs/1706.03762"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e39e29e",
   "metadata": {},
   "source": [
    "An autoregressive model can like NMT be written as follows:\n",
    "\n",
    "\n",
    "$$\n",
    "P(y|x) = P(y_1|x) P(y_2|y_1,x) \\dots P(y_T|y_1, \\dots, y_{T-1}, x)\n",
    "$$\n",
    "\n",
    "where $x$ is the input sequence and $y$ is the output sequence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b14eaf0f",
   "metadata": {},
   "source": [
    "A transformer is better than RNN's in 2 ways:\n",
    "1) Long Range dependencies \n",
    "2) Parallelized approach\n",
    "\n",
    "\n",
    "Allows for transfer learning.\n",
    "This also introduced what attention is. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2d5339de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'en' are deprecated. Please use the\n",
      "full pipeline package name 'en_core_web_sm' instead.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ As of spaCy v3.0, shortcuts like 'de' are deprecated. Please use the\n",
      "full pipeline package name 'de_core_news_sm' instead.\u001b[0m\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('de_core_news_sm')\n"
     ]
    }
   ],
   "source": [
    "# download the spacy language models for english and german\n",
    "!python -m spacy download en --quiet\n",
    "!python -m spacy download de --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5ed0bd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Collecting seaborn\n",
      "  Downloading seaborn-0.12.2-py3-none-any.whl (293 kB)\n",
      "\u001b[K     |████████████████████████████████| 293 kB 5.0 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: numpy!=1.24.0,>=1.17 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.22.3)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.1 in /opt/conda/lib/python3.8/site-packages (from seaborn) (3.5.2)\n",
      "Requirement already satisfied: pandas>=0.25 in /opt/conda/lib/python3.8/site-packages (from seaborn) (1.3.5)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (1.4.2)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (4.33.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (21.3)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (3.0.8)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /opt/conda/lib/python3.8/site-packages (from matplotlib!=3.6.1,>=3.1->seaborn) (9.0.1)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.8/site-packages (from pandas>=0.25->seaborn) (2022.1)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.1->seaborn) (1.16.0)\n",
      "Installing collected packages: seaborn\n",
      "Successfully installed seaborn-0.12.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip3 install seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9731c959",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'Field' from 'torchtext.data' (/opt/conda/lib/python3.8/site-packages/torchtext-0.13.0a0-py3.8-linux-x86_64.egg/torchtext/data/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[0;32mIn [7]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdatasets\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Multi30k\n\u001b[0;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Field, BucketIterator\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchtext\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmetrics\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m bleu_score\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mwarnings\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'Field' from 'torchtext.data' (/opt/conda/lib/python3.8/site-packages/torchtext-0.13.0a0-py3.8-linux-x86_64.egg/torchtext/data/__init__.py)"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rc\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import spacy\n",
    "import torchtext\n",
    "from torchtext.datasets import Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torchtext.data.metrics import bleu_score\n",
    "\n",
    "import warnings\n",
    "from tqdm.notebook import tqdm\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f4f2691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3bb040bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on package torchtext.data in torchtext:\n",
      "\n",
      "NAME\n",
      "    torchtext.data\n",
      "\n",
      "PACKAGE CONTENTS\n",
      "    datasets_utils\n",
      "    functional\n",
      "    metrics\n",
      "    utils\n",
      "\n",
      "FUNCTIONS\n",
      "    bleu_score(candidate_corpus, references_corpus, max_n=4, weights=[0.25, 0.25, 0.25, 0.25])\n",
      "        Computes the BLEU score between a candidate translation corpus and a references\n",
      "        translation corpus. Based on https://www.aclweb.org/anthology/P02-1040.pdf\n",
      "        \n",
      "        Args:\n",
      "            candidate_corpus: an iterable of candidate translations. Each translation is an\n",
      "                iterable of tokens\n",
      "            references_corpus: an iterable of iterables of reference translations. Each\n",
      "                translation is an iterable of tokens\n",
      "            max_n: the maximum n-gram we want to use. E.g. if max_n=3, we will use unigrams,\n",
      "                bigrams and trigrams\n",
      "            weights: a list of weights used for each n-gram category (uniform by default)\n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.data.metrics import bleu_score\n",
      "            >>> candidate_corpus = [['My', 'full', 'pytorch', 'test'], ['Another', 'Sentence']]\n",
      "            >>> references_corpus = [[['My', 'full', 'pytorch', 'test'], ['Completely', 'Different']], [['No', 'Match']]]\n",
      "            >>> bleu_score(candidate_corpus, references_corpus)\n",
      "                0.8408964276313782\n",
      "    \n",
      "    custom_replace(replace_pattern)\n",
      "        A transform to convert text string.\n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.data.functional import custom_replace\n",
      "            >>> custom_replace_transform = custom_replace([(r'S', 's'), (r'\\s+', ' ')])\n",
      "            >>> list_a = [\"Sentencepiece encode  aS  pieces\", \"exampleS to   try!\"]\n",
      "            >>> list(custom_replace_transform(list_a))\n",
      "                ['sentencepiece encode as pieces', 'examples to try!']\n",
      "    \n",
      "    filter_wikipedia_xml(text_iterator)\n",
      "        Filter wikipedia xml lines according to https://github.com/facebookresearch/fastText/blob/master/wikifil.pl\n",
      "        \n",
      "        args:\n",
      "            text_iterator: An iterator type object that yields strings. Examples include string list, text io, generators etc.\n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.data.functional import filter_wikipedia_xml\n",
      "            >>> from torchtext.datasets import EnWik9\n",
      "            >>> data_iter = EnWik9(split='train')\n",
      "            >>> filter_data_iter = filter_wikipedia_xml(data_iter)\n",
      "            >>> file_name = '.data/EnWik9/enwik9'\n",
      "            >>> filter_data_iter = filter_wikipedia_xml(open(file_name,'r'))\n",
      "    \n",
      "    generate_sp_model(filename, vocab_size=20000, model_type='unigram', model_prefix='m_user')\n",
      "        Train a SentencePiece tokenizer.\n",
      "        \n",
      "        Args:\n",
      "            filename: the data file for training SentencePiece model.\n",
      "            vocab_size: the size of vocabulary (Default: 20,000).\n",
      "            model_type: the type of SentencePiece model, including unigram,\n",
      "                bpe, char, word.\n",
      "            model_prefix: the prefix of the files saving model and vocab.\n",
      "        \n",
      "        Outputs:\n",
      "            The model and vocab are saved in two separate files with\n",
      "                model_prefix.\n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.data.functional import generate_sp_model\n",
      "            >>> generate_sp_model('test.csv', vocab_size=23456, model_prefix='spm_user')\n",
      "    \n",
      "    get_tokenizer(tokenizer, language='en')\n",
      "        Generate tokenizer function for a string sentence.\n",
      "        \n",
      "        Args:\n",
      "            tokenizer: the name of tokenizer function. If None, it returns split()\n",
      "                function, which splits the string sentence by space.\n",
      "                If basic_english, it returns _basic_english_normalize() function,\n",
      "                which normalize the string first and split by space. If a callable\n",
      "                function, it will return the function. If a tokenizer library\n",
      "                (e.g. spacy, moses, toktok, revtok, subword), it returns the\n",
      "                corresponding library.\n",
      "            language: Default en\n",
      "        \n",
      "        Examples:\n",
      "            >>> import torchtext\n",
      "            >>> from torchtext.data import get_tokenizer\n",
      "            >>> tokenizer = get_tokenizer(\"basic_english\")\n",
      "            >>> tokens = tokenizer(\"You can now install TorchText using pip!\")\n",
      "            >>> tokens\n",
      "            >>> ['you', 'can', 'now', 'install', 'torchtext', 'using', 'pip', '!']\n",
      "    \n",
      "    interleave_keys(a, b)\n",
      "        Interleave bits from two sort keys to form a joint sort key.\n",
      "        \n",
      "        Examples that are similar in both of the provided keys will have similar\n",
      "        values for the key defined by this function. Useful for tasks with two\n",
      "        text fields like machine translation or natural language inference.\n",
      "    \n",
      "    load_sp_model(spm)\n",
      "        Load a  sentencepiece model for file.\n",
      "        \n",
      "        Args:\n",
      "            spm: the file path or a file object saving the sentencepiece model.\n",
      "        \n",
      "        Outputs:\n",
      "            output: a SentencePiece model.\n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.data.functional import load_sp_model\n",
      "            >>> sp_model = load_sp_model(\"m_user.model\")\n",
      "            >>> sp_model = load_sp_model(open(\"m_user.model\", 'rb'))\n",
      "    \n",
      "    numericalize_tokens_from_iterator(vocab, iterator, removed_tokens=None)\n",
      "        Yield a list of ids from an token iterator with a vocab.\n",
      "        \n",
      "        Args:\n",
      "            vocab: the vocabulary convert token into id.\n",
      "            iterator: the iterator yield a list of tokens.\n",
      "            removed_tokens: removed tokens from output dataset (Default: None)\n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.data.functional import simple_space_split\n",
      "            >>> from torchtext.data.functional import numericalize_tokens_from_iterator\n",
      "            >>> vocab = {'Sentencepiece' : 0, 'encode' : 1, 'as' : 2, 'pieces' : 3}\n",
      "            >>> ids_iter = numericalize_tokens_from_iterator(vocab,\n",
      "            >>>                               simple_space_split([\"Sentencepiece as pieces\",\n",
      "            >>>                                                   \"as pieces\"]))\n",
      "            >>> for ids in ids_iter:\n",
      "            >>>     print([num for num in ids])\n",
      "            >>> [0, 2, 3]\n",
      "            >>> [2, 3]\n",
      "    \n",
      "    sentencepiece_numericalizer(sp_model)\n",
      "        A sentencepiece model to numericalize a text sentence into\n",
      "           a generator over the ids.\n",
      "        \n",
      "        Args:\n",
      "            sp_model: a SentencePiece model.\n",
      "        \n",
      "        Outputs:\n",
      "            output: a generator with the input of text sentence and the output of the\n",
      "                corresponding ids based on SentencePiece model.\n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.data.functional import sentencepiece_numericalizer\n",
      "            >>> sp_id_generator = sentencepiece_numericalizer(sp_model)\n",
      "            >>> list_a = [\"sentencepiece encode as pieces\", \"examples to   try!\"]\n",
      "            >>> list(sp_id_generator(list_a))\n",
      "                [[9858, 9249, 1629, 1305, 1809, 53, 842],\n",
      "                 [2347, 13, 9, 150, 37]]\n",
      "    \n",
      "    sentencepiece_tokenizer(sp_model)\n",
      "        A sentencepiece model to tokenize a text sentence into\n",
      "           a generator over the tokens.\n",
      "        \n",
      "        Args:\n",
      "            sp_model: a SentencePiece model.\n",
      "        \n",
      "        Outputs:\n",
      "            output: a generator with the input of text sentence and the output of the\n",
      "                corresponding tokens based on SentencePiece model.\n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.data.functional import sentencepiece_tokenizer\n",
      "            >>> sp_tokens_generator = sentencepiece_tokenizer(sp_model)\n",
      "            >>> list_a = [\"sentencepiece encode as pieces\", \"examples to   try!\"]\n",
      "            >>> list(sp_tokens_generator(list_a))\n",
      "                [['_sentence', 'piece', '_en', 'co', 'de', '_as', '_pieces'],\n",
      "                 ['_example', 's', '_to', '_try', '!']]\n",
      "    \n",
      "    simple_space_split(iterator)\n",
      "        A transform to split text string by spaces.\n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.data.functional import simple_space_split\n",
      "            >>> list_a = [\"Sentencepiece encode as pieces\", \"example to try!\"]\n",
      "            >>> list(simple_space_split(list_a))\n",
      "                [['Sentencepiece', 'encode', 'as', 'pieces'], ['example', 'to', 'try!']]\n",
      "    \n",
      "    to_map_style_dataset(iter_data)\n",
      "        Convert iterable-style dataset to map-style dataset.\n",
      "        \n",
      "        args:\n",
      "            iter_data: An iterator type object. Examples include Iterable datasets, string list, text io, generators etc.\n",
      "        \n",
      "        \n",
      "        Examples:\n",
      "            >>> from torchtext.datasets import IMDB\n",
      "            >>> from torchtext.data import to_map_style_dataset\n",
      "            >>> train_iter = IMDB(split='train')\n",
      "            >>> train_dataset = to_map_style_dataset(train_iter)\n",
      "            >>> file_name = '.data/EnWik9/enwik9'\n",
      "            >>> data_iter = to_map_style_dataset(open(file_name,'r'))\n",
      "\n",
      "DATA\n",
      "    __all__ = ['bleu_score', 'get_tokenizer', 'interleave_keys', 'generate...\n",
      "\n",
      "FILE\n",
      "    /opt/conda/lib/python3.8/site-packages/torchtext-0.13.0a0-py3.8-linux-x86_64.egg/torchtext/data/__init__.py\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0.13.0a0'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "help(torchtext.data)\n",
    "torchtext.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb27d2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
